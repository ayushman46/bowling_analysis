================================================================================
CRICKET BOWLING 3D ANALYSIS - COMPLETE WORKFLOW EXPLANATION
================================================================================

This document explains the complete working of the Cricket Bowling 3D Analysis
application in detail, covering every step from video upload to final metrics.

================================================================================
TABLE OF CONTENTS
================================================================================

1. PROJECT OVERVIEW
2. FILE STRUCTURE
3. STARTUP SEQUENCE
4. COMPLETE WORKFLOW (Step by Step)
   4.1 Video Upload & Frame Extraction
   4.2 Frame Selection
   4.3 Bowler Detection (YOLO)
   4.4 Pose Detection (MediaPipe)
   4.5 Head Refinement (Face Mesh)
   4.6 3D Mesh Reconstruction (SPIN)
   4.7 Metrics Calculation (from MediaPipe)
   4.8 Visualization & Results
5. KEY ALGORITHMS EXPLAINED
6. DATA FLOW DIAGRAM
7. TROUBLESHOOTING

================================================================================
1. PROJECT OVERVIEW
================================================================================

PURPOSE:
- Analyze cricket bowling action from video
- Detect the bowler (not umpire or batsman)
- Extract 2D pose keypoints
- Generate 3D human mesh
- Calculate biomechanics metrics (elbow angle, legality, etc.)

TECHNOLOGIES USED:
- Streamlit: Web interface
- MediaPipe: 2D pose detection (33 body landmarks)
- YOLO v8: Person detection (to find bowler)
- Face Mesh: Head/face landmark refinement
- SPIN: 3D human mesh reconstruction
- SMPL: Parametric human body model
- Plotly: 3D visualization

================================================================================
2. FILE STRUCTURE
================================================================================

/bowlin/
|
|-- app.py                    # Main Streamlit application
|-- app_config.py             # Configuration (paths, settings)
|-- chumpy_compat.py          # Python 3.12 compatibility patches
|-- requirements.txt          # Python dependencies
|
|-- src/
|   |-- analysis/
|   |   |-- metrics.py        # Bowling metrics calculation
|   |
|   |-- reconstruction/
|   |   |-- spin_wrapper.py   # SPIN model wrapper
|   |
|   |-- visualization/
|   |   |-- render_utils.py   # 3D mesh visualization
|   |
|   |-- ingestion/
|   |   |-- video_utils.py    # Video frame extraction
|   |
|   |-- pose2d/
|   |   |-- mediapipe_runner.py  # MediaPipe pose detection
|   |
|   |-- delivery/
|       |-- delivery_detector.py  # Ball release detection
|
|-- spin_src/                 # SPIN repository (external)
|   |-- models/               # HMR neural network
|   |-- data/                 # Model checkpoints, SMPL models
|
|-- details/                  # Documentation files
    |-- WORKFLOW_EXPLANATION.txt  (this file)

================================================================================
3. STARTUP SEQUENCE
================================================================================

When you run: streamlit run app.py

STEP 1: Python imports and compatibility patches
------------------------------------------------------------------------
File: chumpy_compat.py (imported first)

- Patches inspect.getargspec (removed in Python 3.12)
- Patches numpy type aliases (np.bool, np.object, np.str)
- These patches are needed for the old SMPL/chumpy libraries

STEP 2: Import application modules
------------------------------------------------------------------------
File: app.py (lines 1-40)

- Import Streamlit, OpenCV, NumPy, PIL
- Import custom modules (SpinModelWrapper, metrics, video_utils)
- Import MediaPipe for pose detection

STEP 3: Configure Streamlit page
------------------------------------------------------------------------
File: app.py (line ~430)

- Set page title: "Cricket Bowling 3D Analysis"
- Set layout to "wide" for better visualization

STEP 4: Initialize session state
------------------------------------------------------------------------
File: app.py (function: init_session_state)

Session state variables:
- frames: List of extracted video frames
- frame_indices: Original frame numbers
- timestamps: Time of each frame
- fps: Video frames per second
- selected_frame_idx: Currently selected frame
- accurate_pose: Detected 2D pose landmarks
- analysis_results: 3D mesh and metrics
- analysis_done: Boolean flag

STEP 5: Display UI and wait for user input
------------------------------------------------------------------------
- Show file uploader
- Wait for video upload

================================================================================
4. COMPLETE WORKFLOW (Step by Step)
================================================================================

--------------------------------------------------------------------------------
4.1 VIDEO UPLOAD & FRAME EXTRACTION
--------------------------------------------------------------------------------

Location: app.py, lines ~470-510
Function: extract_frames_from_video() in video_utils.py

WHAT HAPPENS:
1. User uploads video file (.mp4, .mov, .avi)
2. Video is saved to temporary file
3. OpenCV opens the video

FRAME EXTRACTION PROCESS:
```
video = cv2.VideoCapture(temp_path)
fps = video.get(cv2.CAP_PROP_FPS)
total_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)

# Extract every Nth frame (default: every frame up to max_frames)
for i in range(0, total_frames, step):
    video.set(cv2.CAP_PROP_POS_FRAMES, i)
    ret, frame = video.read()
    frames.append(frame)  # BGR format
```

OUTPUT:
- List of frames (NumPy arrays, BGR color)
- Frame indices (original positions)
- Timestamps (in seconds)
- FPS value

--------------------------------------------------------------------------------
4.2 FRAME SELECTION
--------------------------------------------------------------------------------

Location: app.py, lines ~515-560

USER INTERFACE:
- Slider to select frame number
- Thumbnail strip showing frame sequence
- Preview of selected frame

PURPOSE:
- User selects the frame showing the bowling delivery moment
- This is when the bowler's arm is at release point

--------------------------------------------------------------------------------
4.3 BOWLER DETECTION (YOLO)
--------------------------------------------------------------------------------

Location: app.py, function run_ultra_accurate_pose(), lines ~150-200

PROBLEM TO SOLVE:
- Video may contain multiple people (bowler, umpire, batsman, fielders)
- Need to identify which person is the BOWLER

SOLUTION: YOLO + Bowling Action Scoring

STEP 1: Detect all people using YOLO
```python
from ultralytics import YOLO
yolo_model = YOLO("yolov8n.pt")  # Nano model, fast

results = yolo_model(frame, classes=[0])  # class 0 = person
# Returns bounding boxes for all detected people
```

STEP 2: For each detected person, run MediaPipe pose detection

STEP 3: Score each person for "bowling action"
```python
def is_bowling_action(landmarks):
    score = 0
    
    # Wrist above head = bowling action (+500 points)
    if left_wrist[y] < nose[y]:
        score += 500
    if right_wrist[y] < nose[y]:
        score += 500
    
    # Elbow above shoulder = arm raised (+300 points)
    if left_elbow[y] < left_shoulder[y]:
        score += 300
    
    # Arms down = umpire-like pose (-300 points)
    if both_arms_down:
        score -= 300
    
    return score
```

STEP 4: Select person with highest bowling score
- Bowler (arm raised): Score ~1000-2000
- Umpire (standing still): Score ~-200 to 0

--------------------------------------------------------------------------------
4.4 POSE DETECTION (MediaPipe)
--------------------------------------------------------------------------------

Location: app.py, function run_ultra_accurate_pose()

MediaPipe Pose detects 33 body landmarks:

LANDMARK INDICES:
0: NOSE
1-10: Face landmarks (eyes, ears, mouth)
11: LEFT_SHOULDER
12: RIGHT_SHOULDER
13: LEFT_ELBOW
14: RIGHT_ELBOW
15: LEFT_WRIST
16: RIGHT_WRIST
17-22: Hand landmarks
23: LEFT_HIP
24: RIGHT_HIP
25: LEFT_KNEE
26: RIGHT_KNEE
27: LEFT_ANKLE
28: RIGHT_ANKLE
29-32: Foot landmarks

DETECTION PROCESS:
```python
import mediapipe as mp
mp_pose = mp.solutions.pose

with mp_pose.Pose(
    static_image_mode=True,      # Single image, not video
    model_complexity=2,          # Highest accuracy (0, 1, or 2)
    enable_segmentation=False,   # Don't need body mask
    min_detection_confidence=0.3
) as pose:
    result = pose.process(frame_rgb)
    
    # Extract landmarks
    for i, landmark in enumerate(result.pose_landmarks.landmark):
        x = landmark.x * image_width   # Normalized to pixel coords
        y = landmark.y * image_height
        visibility = landmark.visibility  # Confidence 0-1
```

OUTPUT: Dictionary mapping landmark names to (x, y, visibility)
```python
{
    "NOSE": (320.5, 150.2, 0.95),
    "LEFT_SHOULDER": (280.1, 200.5, 0.92),
    "RIGHT_SHOULDER": (360.2, 198.3, 0.91),
    ...
}
```

--------------------------------------------------------------------------------
4.5 HEAD REFINEMENT (Face Mesh)
--------------------------------------------------------------------------------

Location: app.py, function refine_head_landmarks()

PROBLEM:
- MediaPipe Pose is optimized for body, not face
- Head/face landmarks may be inaccurate

SOLUTION: Use Face Mesh for precise face landmarks

PROCESS:
1. Estimate head region from shoulder positions
2. Crop head area from image
3. Run Face Mesh (478 face landmarks)
4. Update head landmarks with precise values

```python
mp_face = mp.solutions.face_mesh

with mp_face.FaceMesh(
    static_image_mode=True,
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.3
) as face_mesh:
    result = face_mesh.process(head_crop)
    
    # Key face mesh landmarks:
    # 1 = nose tip
    # 33 = left eye outer
    # 263 = right eye outer
    # 61 = mouth left
    # 291 = mouth right
```

OUTPUT: Updated landmarks dict with refined head positions

--------------------------------------------------------------------------------
4.6 3D MESH RECONSTRUCTION (SPIN)
--------------------------------------------------------------------------------

Location: src/reconstruction/spin_wrapper.py

SPIN = SMPL Parameter Inference Network

WHAT IT DOES:
- Takes 2D image as input
- Outputs 3D human mesh (6890 vertices, 13776 faces)
- Also outputs SMPL body parameters (pose, shape)

ARCHITECTURE:
```
Input Image (224x224)
       |
       v
  ResNet-50 (Feature Extraction)
       |
       v
  HMR Regressor (Iterative)
       |
       v
  SMPL Parameters:
  - betas (10): Body shape parameters
  - rotmat (24x3x3): Joint rotations
  - camera (3): Scale, translation
       |
       v
  SMPL Model
       |
       v
  Output:
  - vertices (6890x3): 3D mesh vertices
  - joints (49x3): 3D joint positions
  - faces (13776x3): Triangle indices
```

PREPROCESSING:
```python
def _preprocess_image(image_rgb):
    # 1. Enhance contrast (CLAHE)
    lab = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2LAB)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    lab[:, :, 0] = clahe.apply(lab[:, :, 0])
    
    # 2. Resize to 224x224
    img_cropped = cv2.resize(enhanced, (224, 224))
    
    # 3. Normalize (ImageNet mean/std)
    img_tensor = normalize(img_tensor)
    
    return img_tensor
```

SMPL MODEL:
- SMPL = Skinned Multi-Person Linear model
- Parametric model of human body
- Shape controlled by 10 beta parameters
- Pose controlled by 24 joint rotations
- Outputs realistic 3D mesh

```python
smpl_output = self.smpl(
    betas=pred_betas,           # Body shape
    body_pose=pred_rotmat[:, 1:],  # 23 body joint rotations
    global_orient=pred_rotmat[:, 0],  # Root orientation
    pose2rot=False
)

vertices = smpl_output.vertices  # (6890, 3)
joints = smpl_output.joints      # (49, 3)
```

--------------------------------------------------------------------------------
4.7 METRICS CALCULATION (from MediaPipe)
--------------------------------------------------------------------------------

Location: src/analysis/metrics.py
Function: compute_bowling_metrics_from_mediapipe()

WHY MediaPipe (not SPIN)?
- MediaPipe 2D detection is more accurate for the visible pose
- SPIN 3D estimation can be misaligned (wrong arm raised, etc.)
- Cricket metrics depend on accurate angle measurements

METRICS CALCULATED:

1. ELBOW ANGLE (Critical for ICC legality)
```python
def compute_2d_angle(shoulder, elbow, wrist):
    # Vector from elbow to shoulder
    v1 = (shoulder[x] - elbow[x], shoulder[y] - elbow[y])
    
    # Vector from elbow to wrist
    v2 = (wrist[x] - elbow[x], wrist[y] - elbow[y])
    
    # Angle between vectors
    dot_product = v1 . v2
    angle = arccos(dot_product / (|v1| * |v2|))
    
    return degrees(angle)

elbow_angle = compute_2d_angle(shoulder, elbow, wrist)
arm_flexion = 180 - elbow_angle  # How bent the arm is
```

ICC LEGALITY RULES:
- arm_flexion <= 15 degrees: LEGAL
- arm_flexion 15-20 degrees: BORDERLINE
- arm_flexion 20-25 degrees: SUSPICIOUS
- arm_flexion > 25 degrees: ILLEGAL (throwing)

2. RELEASE POINT
```python
# Check if wrist is above shoulder (y-axis inverted in images)
wrist_above = wrist[y] < shoulder[y]
height_diff = shoulder[y] - wrist[y]

if wrist_above and height_diff > 50:
    release_position = "HIGH"  # Good for pace
elif wrist_above:
    release_position = "SHOULDER_HEIGHT"
else:
    release_position = "LOW"  # Spin bowling
```

3. SHOULDER ALIGNMENT
```python
shoulder_slope = (right_shoulder[y] - left_shoulder[y]) / 
                 (right_shoulder[x] - left_shoulder[x])
                 
shoulder_tilt = degrees(arctan(abs(shoulder_slope)))

if shoulder_tilt < 10:
    alignment = "LEVEL"  # Balanced
elif shoulder_tilt < 25:
    alignment = "TILTED"  # Common in pace bowling
else:
    alignment = "HEAVILY_TILTED"  # May cause strain
```

4. HIP-SHOULDER SEPARATION
```python
# Angle of shoulder line
shoulder_angle = arctan2(
    right_shoulder[y] - left_shoulder[y],
    right_shoulder[x] - left_shoulder[x]
)

# Angle of hip line
hip_angle = arctan2(
    right_hip[y] - left_hip[y],
    right_hip[x] - left_hip[x]
)

separation = abs(shoulder_angle - hip_angle)

if separation > 30:
    action_type = "SIDE_ON"  # Classic pace action
elif separation > 15:
    action_type = "SEMI_OPEN"  # Mixed
else:
    action_type = "CHEST_ON"  # Modern pace
```

5. FRONT KNEE ANGLE
```python
front_knee_angle = compute_2d_angle(hip, knee, ankle)

if front_knee_angle > 170:
    front_leg = "BRACED"  # Strong base for pace
elif front_knee_angle > 140:
    front_leg = "SEMI_BRACED"
else:
    front_leg = "BENT"  # May reduce pace
```

6. EFFICIENCY SCORE
```python
score = 50  # Base score

# Add/subtract based on metrics
if legality == "LEGAL": score += 20
if release == "HIGH": score += 15
if front_arm == "EXTENDED": score += 10
if action_type == "SIDE_ON": score += 10
if front_leg == "BRACED": score += 10

# Grade: A (80+), B (65-79), C (50-64), D (35-49), F (<35)
```

--------------------------------------------------------------------------------
4.8 VISUALIZATION & RESULTS
--------------------------------------------------------------------------------

Location: app.py, lines ~750-1000
Location: src/visualization/render_utils.py

A. 2D SKELETON VISUALIZATION
```python
def draw_2d_skeleton_on_frame(frame, pose):
    # Define connections between joints
    connections = [
        ("LEFT_SHOULDER", "LEFT_ELBOW"),
        ("LEFT_ELBOW", "LEFT_WRIST"),
        ("RIGHT_SHOULDER", "RIGHT_ELBOW"),
        ("RIGHT_ELBOW", "RIGHT_WRIST"),
        ...
    ]
    
    # Draw lines between connected joints
    for joint1, joint2 in connections:
        cv2.line(frame, point1, point2, color=(0, 255, 0), thickness=2)
    
    # Draw circles at joint positions
    for name, (x, y, vis) in pose.items():
        cv2.circle(frame, (int(x), int(y)), radius=5, color=(0, 255, 0))
```

B. 3D MESH VISUALIZATION (Plotly)
```python
def make_plotly_mesh_figure(vertices, faces, joints_3d=None):
    # Center the mesh
    center = vertices.mean(axis=0)
    vertices_centered = vertices - center
    
    # Create 3D mesh
    fig = go.Figure(data=[
        go.Mesh3d(
            x=vertices_centered[:, 0],
            y=vertices_centered[:, 1],
            z=vertices_centered[:, 2],
            i=faces[:, 0],  # Triangle vertex indices
            j=faces[:, 1],
            k=faces[:, 2],
            color='rgb(180, 140, 140)',  # Skin tone
            opacity=1.0,
            flatshading=True,
            lighting=dict(ambient=0.5, diffuse=0.8)
        )
    ])
    
    return fig
```

C. METRICS DISPLAY
- Streamlit metrics cards for key values
- Color-coded legality status (green/yellow/red)
- Detailed breakdown in expandable sections
- Coaching insights and recommendations

================================================================================
5. KEY ALGORITHMS EXPLAINED
================================================================================

A. BOWLING ACTION SCORING
--------------------------
Purpose: Distinguish bowler from other people in frame

The algorithm assigns points based on pose characteristics:

BOWLER characteristics:
- Arm raised above head (bowling delivery)
- Dynamic pose (not standing still)
- Larger bounding box (closer to camera, main subject)

UMPIRE characteristics:
- Arms at sides or folded
- Static standing pose
- Often smaller in frame (further away)

B. COORDINATE SYSTEMS
---------------------
Image coordinates (2D):
- Origin: Top-left corner
- X: Left to right (0 to width)
- Y: Top to bottom (0 to height)
- Note: Y increases downward!

SMPL coordinates (3D):
- Origin: Pelvis (root joint)
- X: Left to right
- Y: Bottom to top (opposite of image!)
- Z: Back to front

C. ANGLE CALCULATION
--------------------
All angles calculated using vector dot product:

angle = arccos((v1 . v2) / (|v1| * |v2|))

Where:
- v1, v2 are vectors from the middle joint to outer joints
- Result is in radians, converted to degrees

================================================================================
6. DATA FLOW DIAGRAM
================================================================================

VIDEO FILE
    |
    v
[Frame Extraction] --> List of BGR frames
    |
    v
[User Selection] --> Single frame selected
    |
    v
[YOLO Detection] --> Bounding boxes for all people
    |
    v
[For each person: MediaPipe Pose] --> 2D landmarks
    |
    v
[Bowling Action Scoring] --> Select highest scoring person
    |
    v
[Face Mesh Refinement] --> Refined head landmarks
    |
    v
[SPIN 3D Reconstruction] --> 3D mesh + joints
    |
    v
[Metrics Calculation] <-- Uses MediaPipe 2D (accurate)
    |
    v
[Visualization]
    |
    +---> 2D skeleton overlay
    +---> 3D mesh viewer (Plotly)
    +---> Metrics dashboard
    +---> Coaching insights

================================================================================
7. TROUBLESHOOTING
================================================================================

PROBLEM: "Wrong person detected (umpire instead of bowler)"
SOLUTION: The bowling action scoring was added to fix this.
          System now prefers person with raised arm.

PROBLEM: "Mesh doesn't match 2D pose"
SOLUTION: This is a known limitation. SPIN estimates pose independently.
          Metrics now use MediaPipe 2D for accuracy.

PROBLEM: "Head position is wrong"
SOLUTION: Face Mesh refinement was added to improve head tracking.

PROBLEM: "Import errors with chumpy/SMPL"
SOLUTION: chumpy_compat.py patches Python 3.12 incompatibilities.

PROBLEM: "Model files not found"
SOLUTION: Run setup_spin.py to download required model files.

================================================================================
END OF DOCUMENT
================================================================================
